# ğŸ¤Ÿ SignScribe - A Gesture to Text Translator 

A unique real-time gesture-to-text translation system built using MediaPipe, OpenCV, and scikit-learn. This project lets you collect your own hand gesture data, train a machine learning model, and use it to recognize gestures and convert them into text â€” **your own data, your own gestures, your own model**.

Now also includes a clean **Streamlit-based web interface** (`app.py`) for live gesture prediction. *(Not deployed yet, but locally runnable.)*

---

## âœ¨ Features

- ğŸ¥ Real-time webcam gesture tracking using MediaPipe
- ğŸ–ï¸ Collect custom gesture datasets with labels
- ğŸ§  Train a Random Forest Classifier on your personalized gestures
- ğŸ“– Predict gestures in real-time and convert to text
- ğŸ’» Now includes an interactive **Streamlit GUI** for gesture prediction
- ğŸª¶ Fully Pythonic, minimal dependencies, no deep learning

---

## ğŸ“ Project Structure

```

gesture-to-text/
â”œâ”€â”€ app.py                   # NEW: Streamlit UI for live prediction
â”œâ”€â”€ collect\_data.py          # For collecting gesture samples
â”œâ”€â”€ train\_model.py           # For training ML model
â”œâ”€â”€ predict.py               # For terminal-based real-time prediction
â”œâ”€â”€ data/                    # Folder for gesture .npy data
â”œâ”€â”€ models/
â”‚   â””â”€â”€ gesture\_model.pkl    # Trained model (auto-generated)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore               # Files to ignore in git
â”œâ”€â”€ LICENSE                  # MIT License
â””â”€â”€ README.md                # You're reading this!

````

---

## ğŸ”§ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/dhawalshankar/gesture-to-text.git
cd gesture-to-text
````

### 2. Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

---

## ğŸ¯ Usage Guide

### ğŸ”¹ Step 1: Collect Gesture Data

```bash
python3 collect_data.py
```

* You'll be prompted for gesture name.
* Show your gesture clearly on the webcam.
* 100 samples per gesture will be saved in `data/`.

### ğŸ”¹ Step 2: Train the Model

```bash
python3 train_model.py
```

* Trains a Random Forest classifier using collected gestures
* Saves model as `models/gesture_model.pkl`

### ğŸ”¹ Step 3: Predict Gestures

#### a. Terminal-Based (CLI)

```bash
python3 predict.py
```

#### b. Streamlit App (GUI)

```bash
streamlit run app.py
```

> *(Note: This version is not yet deployed on the cloud, but runs locally.)*

---

## ğŸ“¦ Requirements

```
streamlit
mediapipe
opencv-python
scikit-learn
numpy
joblib
```

Install with:

```bash
pip install -r requirements.txt
```

---

## ğŸ’¡ Future Roadmap

* ğŸ™ï¸ Add text-to-speech feedback (gTTS or pyttsx3)
* ğŸ“ Save gesture logs as subtitle files
* ğŸ”„ Explore time-series models for dynamic gestures
* ğŸŒ Deploy Streamlit app on Vercel/HF/Streamlit Cloud

---

## ğŸ™ Credits

Made with ğŸ’™ by [Dhawal](https://github.com/dhawalshankar),



