# ü§ü SignScribe - A Gesture to Text Translator 

A unique real-time gesture-to-text translation system built using MediaPipe, OpenCV, and scikit-learn. This project lets you collect your own hand gesture data, train a machine learning model, and use it to recognize gestures and convert them into text ‚Äî **your own data, your own gestures, your own model**.

---

## ‚ú® Features

- üé• Real-time webcam gesture tracking using MediaPipe
- üñêÔ∏è Collect multiple gesture datasets with custom labels
- üß† Train a Random Forest Classifier on your personalized data
- üìñ Predict gestures in real time and convert to text
- ü™∂ Fully Pythonic, minimal dependencies, no deep learning

---

## üìÅ Project Structure

```

gesture-to-text/
‚îú‚îÄ‚îÄ collect\_data.py          # For collecting gesture samples
‚îú‚îÄ‚îÄ train\_model.py           # For training ML model
‚îú‚îÄ‚îÄ predict.py               # For real-time prediction
‚îú‚îÄ‚îÄ data/                    # Folder where gesture .npy data is saved
‚îú‚îÄ‚îÄ model.pkl                # Trained model file (auto-generated)
‚îú‚îÄ‚îÄ requirements.txt         # All required libraries
‚îú‚îÄ‚îÄ .gitignore               # Files/folders to ignore in git
‚îú‚îÄ‚îÄ LICENSE                  # MIT License
‚îî‚îÄ‚îÄ README.md                # You're reading this!

````

---

## üîß Setup

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/gesture-to-text.git
cd gesture-to-text
````

### 2. Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

---

## üéØ Usage

### Step 1: Collect Gesture Data

```bash
python3 collect_data.py
```

üëâ You will be prompted to enter a gesture name.
‚úåÔ∏è Show your gesture clearly in the webcam window.
üì¶ 100 samples per gesture are saved as `.npy` files inside `data/`.

> ‚ÑπÔ∏è Run this script separately for each gesture you want to train on.

---

### Step 2: Train the Model

```bash
python3 train_model.py
```

üìö This trains a Random Forest model on your collected data
üíæ Model is saved automatically as `model.pkl`

---

### Step 3: Predict in Real Time

```bash
python3 predict.py
```

üß† Model loads and starts webcam
üñêÔ∏è Show your trained gesture ‚Äî it will print the predicted gesture name

---

## üì¶ Requirements

```txt
mediapipe
opencv-python
scikit-learn
numpy
pandas
matplotlib
```

All installed via:

```bash
pip install -r requirements.txt
```

---

## üîí License

This project is licensed under the [MIT License](LICENSE).

---

## üôè Credits

Made with üíô by [Dhawal Shankar](https://github.com/dhawalshankar),
ECE + CS enthusiast and a devoted learner of Bharatiya values.

> **"‡§∂‡•ç‡§∞‡§Æ ‡§î‡§∞ ‡§∏‡§§‡•ç‡§Ø ‡§∏‡•á ‡§¨‡§®‡§æ ‡§∂‡•ã‡§ß ‡§ï‡§≠‡•Ä ‡§µ‡•ç‡§Ø‡§∞‡•ç‡§• ‡§®‡§π‡•Ä‡§Ç ‡§ú‡§æ‡§§‡§æ‡•§"**

---

## üí° Future Ideas

* Add Text-to-Speech (gTTS or pyttsx3)
* Add GUI with Streamlit or Tkinter
* Export results as subtitles
* Use time-series models for dynamic gestures

````


